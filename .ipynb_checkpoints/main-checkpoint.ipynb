{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coletando notícias do G1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class G1PageScrapper:\n",
    "    \n",
    "    mainUrl = \"https://g1.globo.com/\"\n",
    "    searchPath = \"busca/?q=\"\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "    \n",
    "    def access(self, link):\n",
    "        ret = BeautifulSoup(requests.get(link).content, 'html.parser')\n",
    "        return ret\n",
    "        \n",
    "    def search(self, expression, page=1):\n",
    "        words = expression.split(\" \")\n",
    "        \n",
    "        queryParamValue =  \"\"\n",
    "        \n",
    "        for word in words:\n",
    "            queryParamValue += '+' + word \n",
    "        \n",
    "        queryParamValue = queryParamValue.replace('+','',1)    \n",
    "        \n",
    "        searchUrl = self.mainUrl+self.searchPath+queryParamValue\n",
    "               \n",
    "        return G1SearchReader(self.access(searchUrl))\n",
    "    \n",
    "class G1SearchReader:\n",
    "    \n",
    "    def __init__(self, bsObject):\n",
    "        self.content = bsObject\n",
    "    \n",
    "    def getContent(self):\n",
    "        return self.content\n",
    "    \n",
    "    def listNewsCardList(self):\n",
    "        self.content.find_all(\"ul\") \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "links = ['https://g1.globo.com/rs/rio-grande-do-sul/noticia/2018/08/10/entidade-pede-que-ministerio-publico-do-rs-analise-fala-do-vice-de-bolsonaro-sobre-negros.ghtml',\n",
    "        'https://g1.globo.com/politica/blog/andreia-sadi/post/2018/08/10/apos-debate-morno-estrategistas-de-alckmin-defendem-campanha-na-tv-para-confrontar-bolsonaro.ghtml'\n",
    "        ]\n",
    "\n",
    "webPageScrapper = G1PageScrapper()\n",
    "\n",
    "searchReader = webPageScrapper.search(\"jair bolsonaro\")\n",
    "\n",
    "print(searchReader.listNewsCardList())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = []\n",
    "for link in links:\n",
    "    title, texto = webPageScrapper.access(link)\n",
    "    news_data.append({'title': title, 'content': texto})\n",
    "\n",
    "for news in news_data:\n",
    "    print(news['title'], '\\n\\n', news['content'], '\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculando estatísticas básicas de cada notícia coletada (news_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entidade pede que Ministério Público do RS analise fala do vice de Bolsonaro sobre negros \n",
      "\tNum Sentences:           10\n",
      "\tNum Words:               400\n",
      "\tNum Unique Words:        216\n",
      "\tNum Hapaxes:             167\n",
      "\tTop 10 Most Frequent Words (sans stop words):\n",
      "\t\t entidade (1)\n",
      "\t\teducafro (3)\n",
      "\t\tprotocolou (1)\n",
      "\t\tministério (1)\n",
      "\t\tpúblico (1)\n",
      "\t\trio (2)\n",
      "\t\tgrande (2)\n",
      "\t\tsul (3)\n",
      "\t\tmp-rs (1)\n",
      "\t\tnesta (1)\n",
      "\n",
      "Após debate morno, estrategistas de Alckmin defendem confrontar Bolsonaro na campanha da TV\n",
      "\tNum Sentences:           20\n",
      "\tNum Words:               410\n",
      "\tNum Unique Words:        199\n",
      "\tNum Hapaxes:             142\n",
      "\tTop 10 Most Frequent Words (sans stop words):\n",
      "\t\t primeiro (1)\n",
      "\t\tdebate (3)\n",
      "\t\tpresidencial (1)\n",
      "\t\ttv (3)\n",
      "\t\tdividiu (1)\n",
      "\t\topiniões (1)\n",
      "\t\tcampanhas (2)\n",
      "\t\tprincipais (4)\n",
      "\t\tcandidatos (1)\n",
      "\t\tservirá (1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import nltk\n",
    "\n",
    "\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('portuguese') + [\n",
    "    '.',\n",
    "    ',',\n",
    "    '--',\n",
    "    '\\'s',\n",
    "    '?',\n",
    "    ')',\n",
    "    '(',\n",
    "    ':',\n",
    "    '\\'',\n",
    "    '\\'re',\n",
    "    '\"',\n",
    "    '-',\n",
    "    '}',\n",
    "    '{',\n",
    "    ]\n",
    "\n",
    "for news in news_data:\n",
    "    sentences = nltk.tokenize.sent_tokenize(news['content'])\n",
    "\n",
    "    words = [w.lower() for sentence in sentences for w in\n",
    "             nltk.tokenize.word_tokenize(sentence)]\n",
    "\n",
    "    fdist = nltk.FreqDist(words)\n",
    "\n",
    "    # Basic stats\n",
    "\n",
    "    num_words = sum([i[1] for i in fdist.items()])\n",
    "    num_unique_words = len(fdist.keys())\n",
    "\n",
    "    # Hapaxes are words that appear only once\n",
    "\n",
    "    num_hapaxes = len(fdist.hapaxes())\n",
    "\n",
    "    top_10_words_sans_stop_words = [w for w in fdist.items() if w[0]\n",
    "                                    not in stop_words][:10]\n",
    "\n",
    "    print(news['title'])\n",
    "    print('\\tNum Sentences:'.ljust(25), len(sentences))\n",
    "    print('\\tNum Words:'.ljust(25), num_words)\n",
    "    print('\\tNum Unique Words:'.ljust(25), num_unique_words)\n",
    "    print('\\tNum Hapaxes:'.ljust(25), num_hapaxes)\n",
    "    print('\\tTop 10 Most Frequent Words (sans stop words):\\n\\t\\t', \\\n",
    "            '\\n\\t\\t'.join(['%s (%s)'\n",
    "            % (w[0], w[1]) for w in top_10_words_sans_stop_words]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Igor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sumarizando as notícias coletadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entidade pede que Ministério Público do RS analise fala do vice de Bolsonaro sobre negros \n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "-------------\n",
      "Top N Summary\n",
      "-------------\n",
      "\n",
      " A entidade Educafro protocolou no Ministério Público do Rio Grande do Sul (MP-RS) nesta sexta-feira (10) uma solicitação para que o órgão analise a declaração do general Antonio Hamilton Mourão (PRTB), candidato a vice-presidente na chapa de Jair Bolsonaro (PSL) nas Eleições de 2018, que disse que o Brasil herdou a \"malandragem\" do africano. O político estava em uma reunião-almoço da Câmara de Indústria, Comércio e Serviços de Caxias do Sul, na Serra do Rio Grande do Sul, na segunda-feira (6), quando deu a declaração. No documento encaminhado ao MP, dois dos advogados da Educafro, Ary Bergher e Marcello Ramalho, citam que \"ao vociferar a qualidade negativa do negro na composição cultural do Povo Brasileiro, traz à tona uma aversão aos seus valores e culturas, manifestando, em tese, um racismo não apoiado em bases biológicas, mas sim, numa incompatibilidade entre estes valores e os que sustentam a chamada 'Civilização Ocidental' ao qual o Brasil está incutido\". Na terça-feira (7), entidades repudiaram a fala do vice de Bolsonaro. Também durante o evento, ele havia dito que o Brasil herdou \"indolência\" do índio.\n",
      "\n",
      "-------------------\n",
      "Mean Scored Summary\n",
      "-------------------\n",
      "\n",
      " A entidade Educafro protocolou no Ministério Público do Rio Grande do Sul (MP-RS) nesta sexta-feira (10) uma solicitação para que o órgão analise a declaração do general Antonio Hamilton Mourão (PRTB), candidato a vice-presidente na chapa de Jair Bolsonaro (PSL) nas Eleições de 2018, que disse que o Brasil herdou a \"malandragem\" do africano.\n",
      "\n",
      "Após debate morno, estrategistas de Alckmin defendem confrontar Bolsonaro na campanha da TV\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "-------------\n",
      "Top N Summary\n",
      "-------------\n",
      "\n",
      " O primeiro debate presidencial na TV dividiu opiniões entre as campanhas dos principais candidatos, mas servirá de \"termômetro\" para \"calibrar\" a estratégia no horário eleitoral de Geraldo Alckmin, por exemplo. Aliados de Alckmin admitem que foi calculada a estratégia do tucano de evitar o confronto direto com Jair Bolsonaro, considerado o principal adversário nesta primeira fase. De preferência, afirmam, nas peças em que Alckmin não apareça – os chamados \"spots\" (rápidas inserções de propaganda televisiva). Isso seria especialmente negativo no momento em que o PSDB está desgastado devido às denúncias de corrupção envolvendo seus principais líderes. Avaliam, inclusive, que o candidato do Patriota, Cabo Daciolo, lembrou, na sintonia com Bolsonaro, a dobradinha que Aécio Neves (PSDB) fez com Pastor Everaldo (PSC) em 2014.\n",
      "\n",
      "-------------------\n",
      "Mean Scored Summary\n",
      "-------------------\n",
      "\n",
      " O primeiro debate presidencial na TV dividiu opiniões entre as campanhas dos principais candidatos, mas servirá de \"termômetro\" para \"calibrar\" a estratégia no horário eleitoral de Geraldo Alckmin, por exemplo. Aliados de Alckmin admitem que foi calculada a estratégia do tucano de evitar o confronto direto com Jair Bolsonaro, considerado o principal adversário nesta primeira fase. De preferência, afirmam, nas peças em que Alckmin não apareça – os chamados \"spots\" (rápidas inserções de propaganda televisiva). Avaliam, inclusive, que o candidato do Patriota, Cabo Daciolo, lembrou, na sintonia com Bolsonaro, a dobradinha que Aécio Neves (PSDB) fez com Pastor Everaldo (PSC) em 2014.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import nltk\n",
    "import numpy\n",
    "\n",
    "N = 100  # Number of words to consider\n",
    "CLUSTER_THRESHOLD = 5  # Distance between words to consider\n",
    "TOP_SENTENCES = 5  # Number of sentences to return for a \"top n\" summary\n",
    "\n",
    "# Approach taken from \"The Automatic Creation of Literature Abstracts\" by H.P. Luhn\n",
    "\n",
    "def _score_sentences(sentences, important_words):\n",
    "    scores = []\n",
    "    sentence_idx = -1\n",
    "\n",
    "    for s in [nltk.tokenize.word_tokenize(s) for s in sentences]:\n",
    "\n",
    "        sentence_idx += 1\n",
    "        word_idx = []\n",
    "\n",
    "        # For each word in the word list...\n",
    "        for w in important_words:\n",
    "            try:\n",
    "                # Compute an index for where any important words occur in the sentence\n",
    "\n",
    "                word_idx.append(s.index(w))\n",
    "            except (ValueError) as e: # w not in this particular sentence\n",
    "                pass\n",
    "\n",
    "        word_idx.sort()\n",
    "\n",
    "        # It is possible that some sentences may not contain any important words at all\n",
    "        if len(word_idx)== 0: continue\n",
    "\n",
    "        # Using the word index, compute clusters by using a max distance threshold\n",
    "        # for any two consecutive words\n",
    "\n",
    "        clusters = []\n",
    "        cluster = [word_idx[0]]\n",
    "        i = 1\n",
    "        while i < len(word_idx):\n",
    "            if word_idx[i] - word_idx[i - 1] < CLUSTER_THRESHOLD:\n",
    "                cluster.append(word_idx[i])\n",
    "            else:\n",
    "                clusters.append(cluster[:])\n",
    "                cluster = [word_idx[i]]\n",
    "            i += 1\n",
    "        clusters.append(cluster)\n",
    "\n",
    "        # Score each cluster. The max score for any given cluster is the score \n",
    "        # for the sentence\n",
    "\n",
    "        max_cluster_score = 0\n",
    "        for c in clusters:\n",
    "            significant_words_in_cluster = len(c)\n",
    "            total_words_in_cluster = c[-1] - c[0] + 1\n",
    "            score = 1.0 * significant_words_in_cluster \\\n",
    "                * significant_words_in_cluster / total_words_in_cluster\n",
    "\n",
    "            if score > max_cluster_score:\n",
    "                max_cluster_score = score\n",
    "\n",
    "        scores.append((sentence_idx, score))\n",
    "\n",
    "    return scores\n",
    "\n",
    "def summarize(txt):\n",
    "    sentences = [s for s in nltk.tokenize.sent_tokenize(txt)]\n",
    "    normalized_sentences = [s.lower() for s in sentences]\n",
    "\n",
    "    words = [w.lower() for sentence in normalized_sentences for w in\n",
    "             nltk.tokenize.word_tokenize(sentence)]\n",
    "\n",
    "    fdist = nltk.FreqDist(words)\n",
    "\n",
    "    top_n_words = [w[0] for w in fdist.items() \n",
    "            if w[0] not in nltk.corpus.stopwords.words('portuguese')][:N]\n",
    "\n",
    "    scored_sentences = _score_sentences(normalized_sentences, top_n_words)\n",
    "\n",
    "    # Summaization Approach 1:\n",
    "    # Filter out non-significant sentences by using the average score plus a\n",
    "    # fraction of the std dev as a filter\n",
    "\n",
    "    avg = numpy.mean([s[1] for s in scored_sentences])\n",
    "    std = numpy.std([s[1] for s in scored_sentences])\n",
    "    mean_scored = [(sent_idx, score) for (sent_idx, score) in scored_sentences\n",
    "                   if score > avg + 0.5 * std]\n",
    "\n",
    "    # Summarization Approach 2:\n",
    "    # Another approach would be to return only the top N ranked sentences\n",
    "\n",
    "    top_n_scored = sorted(scored_sentences, key=lambda s: s[1])[-TOP_SENTENCES:]\n",
    "    top_n_scored = sorted(top_n_scored, key=lambda s: s[0])\n",
    "\n",
    "    # Decorate the post object with summaries\n",
    "\n",
    "    return dict(top_n_summary=[sentences[idx] for (idx, score) in top_n_scored],\n",
    "                mean_scored_summary=[sentences[idx] for (idx, score) in mean_scored])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Load in output from blogs_and_nlp__get_feed.py\n",
    "\n",
    "    #BLOG_DATA = sys.argv[1]\n",
    "    #blog_data = json.loads(open(BLOG_DATA).read())\n",
    "    \n",
    "    for post in news_data:\n",
    "       \n",
    "        post.update(summarize(post['content']))\n",
    "\n",
    "        print(post['title'])\n",
    "        print('-' * len(post['title']))\n",
    "        print()\n",
    "        print('-------------')\n",
    "        print('Top N Summary')\n",
    "        print('-------------')\n",
    "        print(' '.join(post['top_n_summary']))\n",
    "        print()\n",
    "        print('-------------------')\n",
    "        print('Mean Scored Summary')\n",
    "        print('-------------------')\n",
    "        print(' '.join(post['mean_scored_summary']))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualização HTML da sumarização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to out\\summarize\\Entidade pede que Ministério Público do RS analise fala do vice de Bolsonaro sobre negros .summary.top_n_summary.html\n",
      "Data written to out\\summarize\\Entidade pede que Ministério Público do RS analise fala do vice de Bolsonaro sobre negros .summary.mean_scored_summary.html\n",
      "Data written to out\\summarize\\Após debate morno, estrategistas de Alckmin defendem confrontar Bolsonaro na campanha da TV.summary.top_n_summary.html\n",
      "Data written to out\\summarize\\Após debate morno, estrategistas de Alckmin defendem confrontar Bolsonaro na campanha da TV.summary.mean_scored_summary.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import nltk\n",
    "import numpy\n",
    "\n",
    "HTML_TEMPLATE = \"\"\"<html>\n",
    "    <head>\n",
    "        <title>%s</title>\n",
    "        <meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"/>\n",
    "    </head>\n",
    "    <body>%s</body>\n",
    "</html>\"\"\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Marked up version can be written out to disk\n",
    "\n",
    "    if not os.path.isdir('out/summarize'):\n",
    "        os.makedirs('out/summarize')\n",
    "\n",
    "    for post in news_data:\n",
    "       \n",
    "        post.update(summarize(post['content']))\n",
    "\n",
    "        for summary_type in ['top_n_summary', 'mean_scored_summary']:\n",
    "            post[summary_type + '_marked_up'] = '<p>%s</p>' % (post['content'], )\n",
    "            for s in post[summary_type]:\n",
    "                post[summary_type + '_marked_up'] = \\\n",
    "                post[summary_type + '_marked_up'].replace(s, '<strong>%s</strong>' % (s, ))\n",
    "\n",
    "            filename = post['title'] + '.summary.' + summary_type + '.html'\n",
    "            f = open(os.path.join('out', 'summarize', filename), 'w')\n",
    "            html = HTML_TEMPLATE % (post['title'] + ' Summary', post[summary_type + '_marked_up'],)\n",
    "            #f.write(str(html.encode('utf-8')))\n",
    "            f.write(html)\n",
    "            f.close()\n",
    "\n",
    "            print(\"Data written to\", f.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
